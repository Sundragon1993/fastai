{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Fast.ai we have introduced a new module called fastai.text which replaces the torchtext library that was used in our 2018 dl1 course. The fastai.text module also supersedes the fastai.nlp library but retains many of the key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fastai.text module introduces several custom tokens.\n",
    "\n",
    "We need to download the IMDB large movie reviews from this site: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "Direct link : [Link](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and untar it into the PATH location. We use pathlib which makes directory traveral a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===================================== (START) Download IMDb data =====================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/aclImdb\n"
     ]
    }
   ],
   "source": [
    "%mkdir data/aclImdb\n",
    "%cd data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#6acf06 79MiB/80MiB(99%) CN:1 DL:\u001b[32m14MiB\u001b[0m]\u001b[0m                          \n",
      "06/26 15:59:49 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /home/ubuntu/data/aclImdb/aclImdb_v1.tar.gz\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "6acf06|\u001b[1;32mOK\u001b[0m  |    14MiB/s|/home/ubuntu/data/aclImdb/aclImdb_v1.tar.gz\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    }
   ],
   "source": [
    "!aria2c --file-allocation=none -c -x 5 -s 5 http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxf aclImdb_v1.tar.gz -C ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "%rm data/aclImdb/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv data/aclImdb/ data/aclImdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv data/aclImdb2/aclImdb data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf data/aclImdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\r\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4.0K Jun 26  2011 .\r\n",
      "drwxrwxr-x 8 ubuntu ubuntu 4.0K Jun 26 16:17 ..\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 882K Jun 11  2011 imdbEr.txt\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 827K Apr 12  2011 imdb.vocab\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4.0K Jun 26  2011 README\r\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4.0K Jun 26 16:02 test\r\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4.0K Jun 26 16:02 train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===================================== (END) Download IMDb data =====================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH = Path('data/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb  dogscats  dogscats.zip  imdb_clas  pascal  spellbee\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_PATH = Path('data/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb  dogscats  dogscats.zip  imdb_clas  imdb_lm  pascal  spellbee\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDb dataset has 3 classes; positive, negative and unsupervised(sentiment is unknown).\n",
    "There are 75k training reviews(12.5k pos, 12.5k neg, 50k unsup)\n",
    "There are 25k validation reviews(12.5k pos, 12.5k neg & no unsup)\n",
    "\n",
    "Refer to the README file in the IMDb corpus for further information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\r\n",
      "\r\n",
      "Overview\r\n",
      "\r\n",
      "This dataset contains movie reviews along with their associated binary\r\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\r\n",
      "sentiment classification. This document outlines how the dataset was\r\n",
      "gathered, and how to use the files provided. \r\n",
      "\r\n",
      "Dataset \r\n",
      "\r\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\r\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\r\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\r\n",
      "documents for unsupervised learning. \r\n",
      "\r\n",
      "In the entire collection, no more than 30 reviews are allowed for any\r\n",
      "given movie because reviews for the same movie tend to have correlated\r\n",
      "ratings. Further, the train and test sets contain a disjoint set of\r\n",
      "movies, so no significant performance is obtained by memorizing\r\n",
      "movie-unique terms and their associated with observed labels.  In the\r\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\r\n",
      "more neutral ratings are not included in the train/test sets. In the\r\n",
      "unsupervised set, reviews of any rating are included and there are an\r\n",
      "even number of reviews > 5 and <= 5.\r\n",
      "\r\n",
      "Files\r\n",
      "\r\n",
      "There are two top-level directories [train/, test/] corresponding to\r\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\r\n",
      "the reviews with binary labels positive and negative. Within these\r\n",
      "directories, reviews are stored in text files named following the\r\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n",
      "the star rating for that review on a 1-10 scale. For example, the file\r\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\r\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\r\n",
      "omitted for this portion of the dataset.\r\n",
      "\r\n",
      "We also include the IMDb URLs for each review in a separate\r\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n",
      "are unable to link directly to the review, but only to the movie's\r\n",
      "review page.\r\n",
      "\r\n",
      "In addition to the review text files, we include already-tokenized bag\r\n",
      "of words (BoW) features that were used in our experiments. These \r\n",
      "are stored in .feat files in the train/test directories. Each .feat\r\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n",
      "data.  The feature indices in these files start from 0, and the text\r\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n",
      "(the) appears 7 times in that review.\r\n",
      "\r\n",
      "LIBSVM page for details on .feat file format:\r\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n",
      "\r\n",
      "We also include [imdbEr.txt] which contains the expected rating for\r\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n",
      "rating is a good way to get a sense for the average polarity of a word\r\n",
      "in the dataset.\r\n",
      "\r\n",
      "Citing the dataset\r\n",
      "\r\n",
      "When using this dataset please cite our ACL 2011 paper which\r\n",
      "introduces it. This paper also contains classification results which\r\n",
      "you may want to compare against.\r\n",
      "\r\n",
      "\r\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\r\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n",
      "  month     = {June},\r\n",
      "  year      = {2011},\r\n",
      "  address   = {Portland, Oregon, USA},\r\n",
      "  publisher = {Association for Computational Linguistics},\r\n",
      "  pages     = {142--150},\r\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n",
      "}\r\n",
      "\r\n",
      "References\r\n",
      "\r\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n",
      "636-659.\r\n",
      "\r\n",
      "Contact\r\n",
      "\r\n",
      "For questions/comments/corrections please contact Andrew Maas\r\n",
      "amaas@cs.stanford.edu\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos', 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path):\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts, trn_labels = get_texts(PATH / 'train')\n",
    "val_texts, val_labels = get_texts(PATH / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 25000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random permutation numpy array to shuffle the text reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({ 'text': trn_texts, 'labels': trn_labels }, columns=col_names)\n",
    "df_val = pd.DataFrame({ 'text': val_texts, 'labels': val_labels }, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of filmmakers (College Students?) deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sequels have a nasty habit of being disappoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a future society, the military component do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Imagine Albert Finney, one of the great ham bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I bought this DVD for $2.00 at the local varie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       2  A group of filmmakers (College Students?) deci...\n",
       "1       0  Sequels have a nasty habit of being disappoint...\n",
       "2       1  In a future society, the military component do...\n",
       "3       2  Imagine Albert Finney, one of the great ham bo...\n",
       "4       2  I bought this DVD for $2.00 at the local varie..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# View train df\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Every year there's one can't-miss much-anticip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't usually like this sort of movie but wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great movie in a Trainspotting style... Being ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>New rule. Nobody is allowed to make any more Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I saw this movie (unfortunately) because it wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       1  Every year there's one can't-miss much-anticip...\n",
       "1       1  I don't usually like this sort of movie but wa...\n",
       "2       1  Great movie in a Trainspotting style... Being ...\n",
       "3       0  New rule. Nobody is allowed to make any more Z...\n",
       "4       0  I saw this movie (unfortunately) because it wa..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# View validation df\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas dataframe is used to store text data in a newly evolving standard format of label followed by text columns. This was influenced by a paper by Yann LeCun (LINK REQUIRED). Fastai adopts this new format for NLP datasets. In the case of IMDB, there is only one text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove everything that has a label of 2 `df_trn['labels'] != 2` because label of 2 is \"unsupervised\" and we can’t use it.\n",
    "df_trn[df_trn['labels'] != 2].to_csv(CLAS_PATH / 'train.csv', header=False, index=False)\n",
    "\n",
    "df_val.to_csv(CLAS_PATH / 'test.csv', header=False, index=False)\n",
    "\n",
    "(CLAS_PATH / 'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the data for the Language Model(LM). The LM's goal is to learn the structure of the English language. It learns language by trying to predict the next word given a set of previous words(ngrams). Since the LM does not classify reviews, the labels can be ignored.\n",
    "\n",
    "The LM can benefit from all the textual data and there is no need to exclude the unsup/unclassified movie reviews.\n",
    "\n",
    "We first concat all the train(pos/neg/unsup = **75k**) and test(pos/neg=**25k**) reviews into a big chunk of **100k** reviews. And then we use sklearn splitter to divide up the 100k texts into 90% training and 10% validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_texts, val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({ 'text': trn_texts, 'labels': [0] * len(trn_texts) }, columns=col_names)\n",
    "df_val = pd.DataFrame({ 'text': val_texts, 'labels': [0] * len(val_texts) }, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH / 'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH / 'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
    "\n",
    "1. Clean up extra spaces, tab chars, new line chars and other characters and replace them with standard ones.\n",
    "2. Use the awesome [spaCy](http://spacy.io) library to tokenize the data. Since spaCy does not provide a parallel/multicore version of the tokenizer, the fastai library adds this functionality. This parallel version uses all the cores of your CPUs and runs much faster than the serial version of the spacy tokenizer.\n",
    "\n",
    "Tokenization is the process of splitting the text into separate tokens so that each token can be assigned a unique index. This means we can convert the text into integer indexes our models can use.\n",
    "\n",
    "We use an appropriate `chunksize` as the tokenization process is memory intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we pass it to spaCy, we will write a simple fixup function which is each time we have looked at different datasets (about a dozen in building this), every one had different weird things that needed to be replaced. So here are all the ones we have come up with so far, and hopefully this will help you out as well. All the entities are HTML unescaped and there are bunch more things we replace. Have a look at the result of running this on text that you put in and make sure there's no more weird tokens in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)):\n",
    "        texts += f' {FLD} {i - n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    \n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    \n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH / 'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH / 'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-51e26f4b98a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtok_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-bfe25ce1655c>\u001b[0m in \u001b[0;36mget_all\u001b[0;34m(df, n_lbls)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtok_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lbls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtok\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtok_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-d4a58d702615>\u001b[0m in \u001b[0;36mget_texts\u001b[0;34m(df, n_lbls)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_all_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_by_cores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl2/fastai/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_br\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<\\s*br\\s*/?>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "# tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix spaCy issue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 81.7MB/s ta 0:00:01  7% |██▎                             | 2.6MB 611kB/s eta 0:00:57\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Re-run these 2 lines of codes.\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH / 'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 90000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tok_trn), len(tok_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'xbos',\n",
       " 'xfld',\n",
       " '1',\n",
       " 'first',\n",
       " 'of',\n",
       " 'all',\n",
       " 'jan',\n",
       " 'guillou',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fantastic',\n",
       " 'writer',\n",
       " '.',\n",
       " 'but',\n",
       " 'even',\n",
       " 'so',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'not',\n",
       " 'read',\n",
       " 'his',\n",
       " '\"',\n",
       " 'arn',\n",
       " '\"-',\n",
       " 'series',\n",
       " 'books',\n",
       " '.',\n",
       " 'as',\n",
       " 'i',\n",
       " 'have',\n",
       " 'great',\n",
       " 'love',\n",
       " 'and',\n",
       " 'respect',\n",
       " 'for',\n",
       " 'guillou',\n",
       " ',',\n",
       " 'i',\n",
       " 'had',\n",
       " 'high',\n",
       " 'expectations',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " '.',\n",
       " 'also',\n",
       " ',',\n",
       " 'a',\n",
       " 'good',\n",
       " 'friend',\n",
       " 'of',\n",
       " 'mine',\n",
       " '(',\n",
       " 'student',\n",
       " 'in',\n",
       " 'university',\n",
       " 'reading',\n",
       " 'history',\n",
       " ')',\n",
       " 'had',\n",
       " 'read',\n",
       " 'and',\n",
       " 'recommended',\n",
       " 'this',\n",
       " 'book',\n",
       " 'strongly',\n",
       " '.',\n",
       " 'perhaps',\n",
       " 'the',\n",
       " 'director',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'catch',\n",
       " 'the',\n",
       " 'atmosphere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'book',\n",
       " ',',\n",
       " 'because',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'was',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'disappointment',\n",
       " '.',\n",
       " 'so',\n",
       " 'i',\n",
       " 'will',\n",
       " 'go',\n",
       " 'very',\n",
       " 'hard',\n",
       " 'on',\n",
       " 'this',\n",
       " 't_up',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'not',\n",
       " 'on',\n",
       " 'the',\n",
       " 'book',\n",
       " 'so',\n",
       " 'please',\n",
       " 'make',\n",
       " 'the',\n",
       " 'difference',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'arn',\n",
       " ',',\n",
       " 'movie',\n",
       " ',',\n",
       " 'tells',\n",
       " 'us',\n",
       " 'the',\n",
       " 'tale',\n",
       " 'of',\n",
       " 'arn',\n",
       " ',',\n",
       " 'born',\n",
       " 'in',\n",
       " '1150',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'north',\n",
       " 'of',\n",
       " 'europe',\n",
       " ',',\n",
       " 'in',\n",
       " 'what',\n",
       " 'would',\n",
       " 'later',\n",
       " 'become',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'sweden',\n",
       " '.',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'basically',\n",
       " 'separated',\n",
       " 'in',\n",
       " 'three',\n",
       " 'parts',\n",
       " ';',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'sweden',\n",
       " ',',\n",
       " 'meaning',\n",
       " 'the',\n",
       " 'rivals',\n",
       " 'and',\n",
       " 'fights',\n",
       " 'for',\n",
       " 'land',\n",
       " 'and',\n",
       " 'kingdom',\n",
       " ',',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'arns',\n",
       " 'own',\n",
       " 'tale',\n",
       " ',',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'a',\n",
       " 'romantic',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'more',\n",
       " 'than',\n",
       " 'that',\n",
       " 'about',\n",
       " 'the',\n",
       " 'movie',\n",
       " ',',\n",
       " 'but',\n",
       " 'now',\n",
       " 'on',\n",
       " 'the',\n",
       " 'trailers',\n",
       " 'you',\n",
       " 'see',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'wars',\n",
       " 'in',\n",
       " 'jerusalem',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'only',\n",
       " 'very',\n",
       " 'short',\n",
       " 'time',\n",
       " 'of',\n",
       " 'this',\n",
       " '2.31',\n",
       " 'hour',\n",
       " 'long',\n",
       " 'movie',\n",
       " '.',\n",
       " '\\n\\n ',\n",
       " 't_up',\n",
       " 'acting',\n",
       " '/',\n",
       " 't_up',\n",
       " 'lines',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'were',\n",
       " 'way',\n",
       " 'too',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'that',\n",
       " 'this',\n",
       " 'was',\n",
       " 'a',\n",
       " 'swedish',\n",
       " 'blockbuster',\n",
       " ',',\n",
       " 'and',\n",
       " 'when',\n",
       " 'they',\n",
       " 'played',\n",
       " 'their',\n",
       " 'rolls',\n",
       " ',',\n",
       " 'one',\n",
       " 'could',\n",
       " 'see',\n",
       " 'an',\n",
       " 'all',\n",
       " 'to',\n",
       " 'relaxed',\n",
       " '(',\n",
       " 'not',\n",
       " 'living',\n",
       " 'into',\n",
       " 'their',\n",
       " 'characters',\n",
       " ')',\n",
       " 'actors',\n",
       " '.',\n",
       " 'acting',\n",
       " 'was',\n",
       " 'so',\n",
       " 'poor',\n",
       " ',',\n",
       " 'i',\n",
       " 'sometimes',\n",
       " 'wondered',\n",
       " ';',\n",
       " 'if',\n",
       " 'this',\n",
       " 't_up',\n",
       " 'was',\n",
       " 'a',\n",
       " 'middle',\n",
       " 'age',\n",
       " 'movie',\n",
       " '.',\n",
       " 'they',\n",
       " 'were',\n",
       " 'saying',\n",
       " 'their',\n",
       " 'lines',\n",
       " 'as',\n",
       " 'a',\n",
       " 'person',\n",
       " 'living',\n",
       " 'in',\n",
       " 'stockholm',\n",
       " 'would',\n",
       " 'do',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'just',\n",
       " 'really',\n",
       " 'lame',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'change',\n",
       " 'the',\n",
       " 'accent',\n",
       " 'i',\n",
       " 'believe',\n",
       " ',',\n",
       " 'made',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'poor',\n",
       " '.',\n",
       " 'in',\n",
       " 'some',\n",
       " 'parts',\n",
       " ',',\n",
       " 'they',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'use',\n",
       " 'old',\n",
       " 'swedish',\n",
       " 'words',\n",
       " '!',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'i',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'believe',\n",
       " 'that',\n",
       " 'the',\n",
       " 'swedish',\n",
       " 'language',\n",
       " '/',\n",
       " 'accent',\n",
       " 'has',\n",
       " \"n't\",\n",
       " 'change',\n",
       " 'since',\n",
       " '1150',\n",
       " '-',\n",
       " '2007',\n",
       " '\\x85 ',\n",
       " 'the',\n",
       " 'lines',\n",
       " 'was',\n",
       " 'empty',\n",
       " 'and',\n",
       " 'when',\n",
       " 'the',\n",
       " 'accents',\n",
       " 'was',\n",
       " 'so',\n",
       " 'poor',\n",
       " ',',\n",
       " 'this',\n",
       " 'equaled',\n",
       " 'in',\n",
       " 'very',\n",
       " 'low',\n",
       " 'performance',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'sometimes',\n",
       " 'felt',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'watching',\n",
       " 'swedish',\n",
       " 'big',\n",
       " 'brother',\n",
       " '\\x85 ',\n",
       " 'i',\n",
       " 'think',\n",
       " 'this',\n",
       " 'is',\n",
       " 'because',\n",
       " 'swedish',\n",
       " 'actors',\n",
       " 'has',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'understood',\n",
       " 'that',\n",
       " 'acting',\n",
       " 'is',\n",
       " 'with',\n",
       " 'whole',\n",
       " 'body',\n",
       " ',',\n",
       " 'eye',\n",
       " 'moves',\n",
       " ',',\n",
       " 'body',\n",
       " '-',\n",
       " 'language',\n",
       " ',',\n",
       " 'etc',\n",
       " ',',\n",
       " 'not',\n",
       " 'just',\n",
       " 'standing',\n",
       " 'there',\n",
       " 'like',\n",
       " 'a',\n",
       " 'jukebox',\n",
       " 'and',\n",
       " 'saying',\n",
       " 'your',\n",
       " 'lines',\n",
       " 'one',\n",
       " 'after',\n",
       " 'other',\n",
       " '.',\n",
       " 'the',\n",
       " 'kids',\n",
       " 'acting',\n",
       " 'was',\n",
       " 'horrible',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'dying',\n",
       " 'every',\n",
       " 'time',\n",
       " 'they',\n",
       " 'said',\n",
       " 'their',\n",
       " 'empty',\n",
       " 'lines',\n",
       " ',',\n",
       " 'with',\n",
       " 'no',\n",
       " 'feeling',\n",
       " ',',\n",
       " 'just',\n",
       " 'saying',\n",
       " 'it',\n",
       " 'as',\n",
       " 'instructed',\n",
       " '!',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'compare',\n",
       " 'little',\n",
       " 'girl',\n",
       " 'briony',\n",
       " 'tallis',\n",
       " 'in',\n",
       " 'atonement',\n",
       " ',',\n",
       " 'her',\n",
       " 'majestic',\n",
       " 'way',\n",
       " 'of',\n",
       " 'acting',\n",
       " ',',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'crowd',\n",
       " ',',\n",
       " 'filling',\n",
       " 'up',\n",
       " 'the',\n",
       " 'scenes',\n",
       " 'whether',\n",
       " 'to',\n",
       " 'make',\n",
       " 'us',\n",
       " 'like',\n",
       " 'her',\n",
       " ',',\n",
       " 'hate',\n",
       " 'her',\n",
       " ',',\n",
       " 'moving',\n",
       " 'us',\n",
       " 'from',\n",
       " 'different',\n",
       " 'moods',\n",
       " 'by',\n",
       " 'her',\n",
       " 'acting',\n",
       " ',',\n",
       " 'those',\n",
       " 'well',\n",
       " '-',\n",
       " 'formed',\n",
       " 'lines',\n",
       " 'she',\n",
       " 'so',\n",
       " 'realistically',\n",
       " 'with',\n",
       " 'so',\n",
       " 'much',\n",
       " 'emotions',\n",
       " 'said',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'find',\n",
       " 'her',\n",
       " 'too',\n",
       " 'old',\n",
       " 'for',\n",
       " 'comparative',\n",
       " 'purpose',\n",
       " ',',\n",
       " 'then',\n",
       " 'one',\n",
       " 'could',\n",
       " 'compare',\n",
       " 'to',\n",
       " 'jake',\n",
       " 'lloyd',\n",
       " ',',\n",
       " 'little',\n",
       " 'anakin',\n",
       " 'skywalker',\n",
       " '(',\n",
       " 'sw',\n",
       " 'e1',\n",
       " ')',\n",
       " '.',\n",
       " 'in',\n",
       " 'arn',\n",
       " ',',\n",
       " 'those',\n",
       " 'kids',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'have',\n",
       " 'many',\n",
       " 'scenes',\n",
       " 'to',\n",
       " 'play',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'their',\n",
       " 'scenes',\n",
       " 'where',\n",
       " 'just',\n",
       " 'the',\n",
       " 'same',\n",
       " ',',\n",
       " 'and',\n",
       " 'lines',\n",
       " 'so',\n",
       " 'easy',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'jakes',\n",
       " ',',\n",
       " 'whom',\n",
       " 'was',\n",
       " 'the',\n",
       " 'one',\n",
       " 'character',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'waited',\n",
       " 'for',\n",
       " 'since',\n",
       " '1977',\n",
       " '!',\n",
       " 'the',\n",
       " 'only',\n",
       " 'good',\n",
       " 'actors',\n",
       " 'in',\n",
       " 'arn',\n",
       " 'was',\n",
       " 'the',\n",
       " 'old',\n",
       " 'nun',\n",
       " ',',\n",
       " 'mother',\n",
       " 'rikissa',\n",
       " ',',\n",
       " 'she',\n",
       " 'was',\n",
       " '\"',\n",
       " 'good',\n",
       " '\"',\n",
       " 'but',\n",
       " 'not',\n",
       " 'perfect',\n",
       " 'as',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'should',\n",
       " 'have',\n",
       " 'been',\n",
       " 'in',\n",
       " 'this',\n",
       " 'movie',\n",
       " '!',\n",
       " 'also',\n",
       " 'saladin',\n",
       " ',',\n",
       " 'father',\n",
       " 'henry',\n",
       " ',',\n",
       " 'brother',\n",
       " 'guilbert',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'bishop',\n",
       " 'passed',\n",
       " ',',\n",
       " 'all',\n",
       " 'other',\n",
       " 'sucked',\n",
       " 'so',\n",
       " 'bad',\n",
       " 'i',\n",
       " 'was',\n",
       " 'about',\n",
       " 'to',\n",
       " 'leave',\n",
       " 'the',\n",
       " 'cinema',\n",
       " 'and',\n",
       " 'take',\n",
       " 'a',\n",
       " 'rape',\n",
       " '-',\n",
       " 'shower',\n",
       " '.',\n",
       " 'worst',\n",
       " 'acting',\n",
       " 'i',\n",
       " 'found',\n",
       " ',',\n",
       " 'cecilia',\n",
       " 'algottsdotter',\n",
       " '(',\n",
       " 'and',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " ')',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'no',\n",
       " 'words',\n",
       " '.',\n",
       " 'arn',\n",
       " 'himself',\n",
       " 'is',\n",
       " 'right',\n",
       " 'behind',\n",
       " '\\x85 ',\n",
       " 'they',\n",
       " 'could',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'let',\n",
       " 'us',\n",
       " 'feel',\n",
       " 'those',\n",
       " 'really',\n",
       " 'important',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'creating',\n",
       " 'a',\n",
       " 'character',\n",
       " ',',\n",
       " 'make',\n",
       " 'us',\n",
       " 'love',\n",
       " ',',\n",
       " 'hate',\n",
       " ',',\n",
       " 'or',\n",
       " 'mystic',\n",
       " 'feelings',\n",
       " '.',\n",
       " '\\n\\n ',\n",
       " 't_up',\n",
       " 'cutting',\n",
       " '/',\n",
       " 't_up',\n",
       " 'camera',\n",
       " '(',\n",
       " 'story',\n",
       " 'telling',\n",
       " ')',\n",
       " 'what',\n",
       " 'were',\n",
       " 'those',\n",
       " 'stupid',\n",
       " 'layering',\n",
       " 'other',\n",
       " 'scenes',\n",
       " 'about',\n",
       " '(',\n",
       " 'picture',\n",
       " 'over',\n",
       " 'picture',\n",
       " ')',\n",
       " '!',\n",
       " '?',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'when',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'create',\n",
       " 'a',\n",
       " 'flashback',\n",
       " ',',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " 'good',\n",
       " ',',\n",
       " 'like',\n",
       " '\"',\n",
       " 'i',\n",
       " 'am',\n",
       " 'legend',\n",
       " '\"',\n",
       " '.',\n",
       " 'even',\n",
       " 'the',\n",
       " 'tv',\n",
       " '-',\n",
       " 'series',\n",
       " 't_up',\n",
       " 'lost',\n",
       " 'makes',\n",
       " 'better',\n",
       " 'and',\n",
       " 'more',\n",
       " 'intense',\n",
       " 'flashbacks',\n",
       " '.',\n",
       " 'and',\n",
       " 'most',\n",
       " 'times',\n",
       " 'this',\n",
       " 'layering',\n",
       " 'of',\n",
       " 'flags',\n",
       " 'etc',\n",
       " 'just',\n",
       " 'explains',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'watcher',\n",
       " 'as',\n",
       " 'stupid',\n",
       " '!',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'in',\n",
       " 'one',\n",
       " 'scene',\n",
       " ',',\n",
       " 'mother',\n",
       " 'rikissa',\n",
       " 'says',\n",
       " '\"',\n",
       " 'we',\n",
       " 'are',\n",
       " 'sverkar',\n",
       " '\"',\n",
       " 'and',\n",
       " 't_up',\n",
       " 'bam',\n",
       " 'a',\n",
       " 'layer',\n",
       " 'over',\n",
       " 'her',\n",
       " 'face',\n",
       " 'with',\n",
       " 'their',\n",
       " 'flag',\n",
       " ',',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'meaning',\n",
       " '?',\n",
       " '!',\n",
       " 'who',\n",
       " 'made',\n",
       " 'that',\n",
       " 'on',\n",
       " 'photoshop',\n",
       " '?',\n",
       " '!',\n",
       " 'i',\n",
       " 'will',\n",
       " 'come',\n",
       " 'to',\n",
       " 'that',\n",
       " 'later',\n",
       " '!',\n",
       " 'the',\n",
       " 'cutting',\n",
       " 'was',\n",
       " 'poor',\n",
       " ',',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'scenes',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'make',\n",
       " 'sense',\n",
       " 'at',\n",
       " 'all',\n",
       " ',',\n",
       " 'we',\n",
       " 'were',\n",
       " 'pushed',\n",
       " 'all',\n",
       " 'too',\n",
       " 'suddenly',\n",
       " 'from',\n",
       " 'one',\n",
       " 'place',\n",
       " 'to',\n",
       " 'another',\n",
       " 'and',\n",
       " 'we',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'get',\n",
       " 'an',\n",
       " 'explanation',\n",
       " 'why',\n",
       " '\\x85 ',\n",
       " 'arns',\n",
       " 'riding',\n",
       " '(',\n",
       " 'i',\n",
       " 'even',\n",
       " 'heard',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'is',\n",
       " 'actually',\n",
       " 'good',\n",
       " 'at',\n",
       " 'riding',\n",
       " ')',\n",
       " 'was',\n",
       " 'really',\n",
       " 'bad',\n",
       " ',',\n",
       " 'the',\n",
       " 'horse',\n",
       " 'was',\n",
       " 'so',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'powerful',\n",
       " ',',\n",
       " 'but',\n",
       " 'arns',\n",
       " 'carriage',\n",
       " '/',\n",
       " 'attitude',\n",
       " 'made',\n",
       " 'it',\n",
       " 'look',\n",
       " 'so',\n",
       " 't_up',\n",
       " 'stupid',\n",
       " ',',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'a',\n",
       " 'cool',\n",
       " 'horse',\n",
       " 'riding',\n",
       " 'scenes',\n",
       " 'have',\n",
       " 'we',\n",
       " 'all',\n",
       " 'seen',\n",
       " ',',\n",
       " 'is',\n",
       " 'it',\n",
       " 'so',\n",
       " 'hard',\n",
       " '?',\n",
       " '!',\n",
       " 'in',\n",
       " 'elizabeth',\n",
       " 'the',\n",
       " 'golden',\n",
       " 'age',\n",
       " ',',\n",
       " 'when',\n",
       " 'they',\n",
       " 'ride',\n",
       " 'their',\n",
       " 'horses',\n",
       " ',',\n",
       " 'one',\n",
       " 'catches',\n",
       " 'that',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'speed',\n",
       " ',',\n",
       " 'carriage',\n",
       " '/',\n",
       " 'attitude',\n",
       " 'i',\n",
       " 'am',\n",
       " 'talking',\n",
       " 'about',\n",
       " '.',\n",
       " '\"',\n",
       " 'let',\n",
       " 'them',\n",
       " 'come',\n",
       " 'with',\n",
       " 'armies',\n",
       " 'of',\n",
       " 'hell',\n",
       " '!',\n",
       " 'they',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'pass',\n",
       " '!',\n",
       " '\"',\n",
       " 'elizabeth',\n",
       " 'shouts',\n",
       " 'with',\n",
       " 'furious',\n",
       " 'anger',\n",
       " '!',\n",
       " 'cool',\n",
       " 'quotes',\n",
       " 'as',\n",
       " 'these',\n",
       " 'can',\n",
       " 'also',\n",
       " 'be',\n",
       " 'found',\n",
       " '(',\n",
       " 'in',\n",
       " 'elizabeth',\n",
       " '\\x96',\n",
       " 'not',\n",
       " 'arn',\n",
       " '!',\n",
       " 'not',\n",
       " 'one',\n",
       " 'single',\n",
       " 'line',\n",
       " 'is',\n",
       " 'memorable',\n",
       " 'quote',\n",
       " '!',\n",
       " ')',\n",
       " 'i',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'like',\n",
       " 'elizabeth',\n",
       " 'the',\n",
       " 'golden',\n",
       " 'age',\n",
       " '!',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 90000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_labels), len(trn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Result\n",
    "\n",
    "Beginning of the stream token (`xbos`), beginning of field number 1 token (`xfld 1`), and tokenized text. You'll see that the punctuation is on whole now a separate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n xbos xfld 1 first of all jan guillou is a fantastic writer . but even so , i have not read his \" arn \"- series books . as i have great love and respect for guillou , i had high expectations for this movie . also , a good friend of mine ( student in university reading history ) had read and recommended this book strongly . perhaps the director could n\\'t catch the atmosphere in the book , because the movie was a huge disappointment . so i will go very hard on this t_up movie and not on the book so please make the difference . \\n\\n arn , movie , tells us the tale of arn , born in 1150 , in the north of europe , in what would later become today \\'s sweden . the movie is basically separated in three parts ; ( 1 ) rise of sweden , meaning the rivals and fights for land and kingdom , ( 2 ) arns own tale , ( 3 ) a romantic . i do n\\'t want to tell more than that about the movie , but now on the trailers you see a lot of wars in jerusalem , but that is only very short time of this 2.31 hour long movie . \\n\\n  t_up acting / t_up lines the actors were way too aware of that this was a swedish blockbuster , and when they played their rolls , one could see an all to relaxed ( not living into their characters ) actors . acting was so poor , i sometimes wondered ; if this t_up was a middle age movie . they were saying their lines as a person living in stockholm would do in 2007 , which was just really lame , there was no attempt to change the accent i believe , made the quality poor . in some parts , they did n\\'t even use old swedish words ! i mean i ca n\\'t believe that the swedish language / accent has n\\'t change since 1150 - 2007 \\x85  the lines was empty and when the accents was so poor , this equaled in very low performance , and i sometimes felt that i was watching swedish big brother \\x85  i think this is because swedish actors has not yet understood that acting is with whole body , eye moves , body - language , etc , not just standing there like a jukebox and saying your lines one after other . the kids acting was horrible , i was dying every time they said their empty lines , with no feeling , just saying it as instructed ! i mean , compare little girl briony tallis in atonement , her majestic way of acting , taking the crowd , filling up the scenes whether to make us like her , hate her , moving us from different moods by her acting , those well - formed lines she so realistically with so much emotions said . if you find her too old for comparative purpose , then one could compare to jake lloyd , little anakin skywalker ( sw e1 ) . in arn , those kids did n\\'t even have many scenes to play and most of their scenes where just the same , and lines so easy compared to jakes , whom was the one character we \\'ve waited for since 1977 ! the only good actors in arn was the old nun , mother rikissa , she was \" good \" but not perfect as the actors should have been in this movie ! also saladin , father henry , brother guilbert , and the bishop passed , all other sucked so bad i was about to leave the cinema and take a rape - shower . worst acting i found , cecilia algottsdotter ( and her sister in the movie ) , i have no words . arn himself is right behind \\x85  they could simply not let us feel those really important scenes of creating a character , make us love , hate , or mystic feelings . \\n\\n  t_up cutting / t_up camera ( story telling ) what were those stupid layering other scenes about ( picture over picture ) ! ? i mean , when you \\'re supposed to create a flashback , you want to do it good , like \" i am legend \" . even the tv - series t_up lost makes better and more intense flashbacks . and most times this layering of flags etc just explains the movie watcher as stupid ! i mean in one scene , mother rikissa says \" we are sverkar \" and t_up bam a layer over her face with their flag , what is the meaning ? ! who made that on photoshop ? ! i will come to that later ! the cutting was poor , a lot of scenes did n\\'t make sense at all , we were pushed all too suddenly from one place to another and we did n\\'t even get an explanation why \\x85  arns riding ( i even heard this guy is actually good at riding ) was really bad , the horse was so beautiful and powerful , but arns carriage / attitude made it look so t_up stupid , i mean a cool horse riding scenes have we all seen , is it so hard ? ! in elizabeth the golden age , when they ride their horses , one catches that freedom , speed , carriage / attitude i am talking about . \" let them come with armies of hell ! they shall not pass ! \" elizabeth shouts with furious anger ! cool quotes as these can also be found ( in elizabeth \\x96 not arn ! not one single line is memorable quote ! ) i did n\\'t even like elizabeth the golden age ! \\n\\n  t_up atmosphere / t_up music now , some movies have characteristic music , godfather , star wars , lord of the rings , every movie you read now you \\'ve heard the music and felt that atmosphere within you , did n\\'t you ? forget that on arn . the music is really poor , no atmosphere at all , no tension or bravery \\x96 there is nothing . and i mean , this is the type of movie where you could create amazing music to create different moods ! \\n\\n  t_up environment well , this was the only positive part of the movie , beautiful places , great houses , roads , weapons and clothes , all that was really good . \\n\\n so my conclusion is . they have tried to push three books in one movie and therefore the cutting , building up characters , story , conflicts and lines has failed brutally . after watching this movie you will see : bad actors , no good lines , extremely bad cutting ( read bad story telling ) and no good music . simply no atmosphere , no creation of different moods etc and is n\\'t that what a movie is suppose to do ? i rate arn 1 awful . \" kingdom of heaven \" still is winner in this genre .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n xbos xfld 1 for lack of better things to watch , we stumbled on this movie the other night on cable . wow ! if action is your thing , this film will be for you . there must be killings every five minutes . in fact , we are worried when there are no shootings in the background ! \\n\\n charles t. kanganis wrote and directed this movie that has a woman detective at the center of the story . vickie , is a tough cookie ( no pun intended ) . she might look blonde and vulnerable , but just do n't mess around with her . the fact that vickie is basically standing up as the film ends is a testament to tracy lords ' masochism . \\n\\n the bad guys come and go , yet , vickie is able to avoid being shot , or have her hair messed during the worst of the action . the action is too intense at times as the latin gangsters show to be ruthless in the way they settle disputes . \\n\\n watch this film for the pure fun of watching the action . otherwise , do n't bother .\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH / 'tmp' / 'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH / 'tmp' / 'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load it back up later\n",
    "tok_trn = np.load(LM_PATH / 'tmp' / 'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH / 'tmp' / 'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 381M\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1010K Jun 27 15:42 itos.pkl\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  299M Jun 28 07:30 tok_trn.npy\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   34M Jun 28 07:30 tok_val.npy\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   42M Jun 27 15:42 trn_ids.npy\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  6.3M Jun 27 15:42 val_ids.npy\r\n"
     ]
    }
   ],
   "source": [
    "# DEBUG - check serialized numpy files are created\n",
    "!ls -lh {LM_PATH}/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got it tokenized, the next thing we need to do is to turn it into numbers which we call numericalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1209252),\n",
       " ('.', 993276),\n",
       " (',', 984626),\n",
       " ('and', 587546),\n",
       " ('a', 584199),\n",
       " ('of', 524706),\n",
       " ('to', 485996),\n",
       " ('is', 394082),\n",
       " ('it', 341931),\n",
       " ('in', 337846),\n",
       " ('i', 308713),\n",
       " ('this', 270873),\n",
       " ('that', 261496),\n",
       " ('\"', 237503),\n",
       " (\"'s\", 221557),\n",
       " ('-', 187727),\n",
       " ('was', 180500),\n",
       " ('\\n\\n', 179162),\n",
       " ('as', 166379),\n",
       " ('with', 159369),\n",
       " ('for', 159204),\n",
       " ('movie', 157938),\n",
       " ('but', 150477),\n",
       " ('film', 144038),\n",
       " ('you', 124418)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *vocab* is the **unique set of all tokens** in our dataset. The vocab provides us a way for us to simply replace each word in our datasets with a unique integer called an index.\n",
    "\n",
    "In a large corpus of data one might find some rare words which are only used a few times in the whole dataset. We discard such rare words and avoid trying to learn meaningful patterns out of them.\n",
    "\n",
    "Here we have set a minimum frequency of occurence to 2 times. It has been observed by NLP practicioners that a maximum vocab of 60k usually yields good results for classification tasks. So we set `max_vocab` to 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a reverse mapping called `stoi` which is useful to lookup the index of a given token. `stoi` also has the same number of elements as `itos`. We use a high performance container called [collections.defaultdict](https://docs.python.org/2/library/collections.html#collections.defaultdict) to store our `stoi` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60002"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = collections.defaultdict(lambda: 0, { v: k for k, v in enumerate(itos) })\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([ [stoi[o] for o in p] for p in tok_trn ])\n",
    "val_lm = np.array([ [stoi[o] for o in p] for p in tok_val ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40 41 42 39 106 7 43 5033 36163 9 6 846 569 3 24 75 51 4 12 36 32 369 35 15 18231 5213 228 1218 3 20 12 36 101 131 5 1166 22 36163 4 12 84 323 1413 22 13 23 3 102 4 6 66 443 7 1805 30 1374 11 3458 901 499 27 84 369 5 1169 13 309 2337 3 400 2 169 95 29 1323 2 879 11 2 309 4 105 2 23 18 6 672 1459 3 51 12 104 158 69 266 28 13 31 23 5 32 28 2 309 51 617 112 2 1540 3 19 18231 4 23 4 716 200 2 777 7 18231 4 1362 11 0 4 11 2 2135 7 2258 4 11 63 72 329 445 511 16 7852 3 2 23 9 681 5744 11 300 522 133 30 39 27 2240 7 7852 4 1201 2 6910 5 1758 22 1300 5 4683 4 30 261 27 58729 221 777 4 30 381 27 6 745 3 12 57 29 201 8 401 67 92 14 58 2 23 4 24 166 28 2 4351 26 83 6 186 7 1610 11 16640 4 24 14 9 81 69 365 74 7 13 0 562 216 23 3 640 31 136 123 31 428 2 170 85 115 116 1864 7 14 13 18 6 3630 2673 4 5 68 45 275 78 6284 4 38 95 83 47 43 8 7992 30 32 593 103 78 120 27 170 3 136 18 51 357 4 12 557 3661 133 62 13 31 18 6 658 603 23 3 45 85 660 78 428 20 6 417 593 11 19313 72 57 11 4216 4 79 18 56 82 871 4 53 18 73 590 8 678 2 1231 12 283 4 113 2 503 357 3 11 64 522 4 45 86 29 75 376 175 3630 690 49 12 403 12 195 29 283 14 2 3630 1090 123 1231 60 29 678 252 0 17 4216 1623 2 428 18 1887 5 68 2 2605 18 51 357 4 13 20935 11 69 380 257 4 5 12 557 462 14 12 18 168 3630 219 607 1623 12 121 13 9 105 3630 170 60 32 262 2769 14 136 9 21 238 662 4 766 1124 4 662 17 1090 4 528 4 32 56 2022 53 52 6 30104 5 660 146 428 38 118 100 3 2 378 136 18 510 4 12 18 1642 189 74 45 325 78 1887 428 4 21 73 574 4 56 660 10 20 16641 49 12 403 4 1740 138 256 0 0 11 22523 4 55 11340 115 7 136 4 655 2 2031 4 6111 71 2 155 768 8 112 200 52 55 4 738 55 4 763 200 50 285 10177 46 55 136 4 164 88 17 6304 428 70 51 6340 21 51 93 1361 325 3 62 26 185 55 116 175 22 23472 1264 4 114 38 95 1740 8 3534 2585 4 138 13179 13552 30 23473 0 27 3 11 18231 4 164 378 86 29 75 36 128 155 8 315 5 109 7 78 155 134 56 2 187 4 5 428 51 749 1111 8 24947 4 938 18 2 38 122 89 159 4798 22 252 5991 49 2 81 66 170 11 18231 18 2 175 5761 4 399 0 4 70 18 15 66 15 24 32 424 20 2 170 154 36 98 11 13 23 49 102 28259 4 355 1370 4 607 44777 4 5 2 6489 2220 4 43 100 2125 51 96 12 18 58 8 577 2 458 5 213 6 1796 17 3450 3 269 136 12 276 4 10124 0 30 5 55 802 11 2 23 27 4 12 36 73 690 3 18231 333 9 227 538 1623 45 95 346 32 302 200 250 164 82 648 155 7 1853 6 122 4 112 200 131 4 738 4 54 11341 1427 3 640 31 2227 123 31 375 30 80 983 27 63 85 164 394 29173 100 155 58 30 450 141 450 27 49 65 12 403 4 68 26 198 457 8 1016 6 2454 4 26 201 8 57 10 66 4 52 15 12 260 1857 15 3 75 2 264 17 228 31 436 177 145 5 67 1597 2078 3 5 109 235 13 29173 7 11420 528 56 2676 2 23 8260 20 394 49 12 403 11 38 149 4 399 0 566 15 89 33 0 15 5 31 8261 6 8388 141 55 410 21 78 5265 4 63 9 2 1201 65 49 48 113 14 28 34658 65 49 12 104 232 8 14 329 49 2 2227 18 357 4 6 186 7 155 86 29 112 291 44 43 4 89 85 3981 43 116 1089 50 38 286 8 174 5 89 86 29 75 97 47 1822 153 1623 58729 2894 30 12 75 572 13 230 9 179 66 44 2894 27 18 82 96 4 2 1703 18 51 342 5 940 4 24 58729 9884 123 2054 113 10 184 51 31 394 4 12 403 6 601 1703 2894 155 36 89 43 129 4 9 10 51 266 65 49 11 2450 2 2102 603 4 68 45 1200 78 3553 4 38 4447 14 2105 4 2205 4 9884 123 2054 12 260 687 58 3 15 302 110 232 21 12509 7 582 49 45 3542 32 1383 49 15 2450 9527 21 5805 2674 49 601 4770 20 150 77 102 37 276 30 11 2450 526 32 18231 49 32 38 684 367 9 919 3167 49 27 12 86 29 75 52 2450 2 2102 603 49 640 31 879 123 31 239 166 4 64 117 36 8465 239 4 3717 4 341 1610 4 1737 7 2 2959 4 189 23 26 369 166 26 159 572 2 239 5 462 14 879 748 26 4 86 29 26 65 838 14 28 18231 3 2 239 9 82 357 4 73 879 44 43 4 73 1122 54 10178 526 53 9 176 3 5 12 403 4 13 9 2 545 7 23 134 26 95 1016 525 239 8 1016 285 10177 49 640 31 2788 88 4 13 18 2 81 1173 191 7 2 23 4 342 1293 4 101 4789 4 8814 4 2499 5 1749 4 43 14 18 82 66 3 19 51 76 1207 9 3 45 36 792 8 3315 300 1218 11 38 23 5 1575 2 2227 4 1280 71 120 4 80 4 4820 5 428 60 1213 4301 3 118 168 13 23 26 104 83 94 96 170 4 73 66 428 4 578 96 2227 30 369 96 80 983 27 5 73 66 239 3 346 73 879 4 73 3159 7 285 10177 528 5 9 29 14 63 6 23 9 1363 8 57 65 12 1001 18231 39 398 3 15 4683 7 2120 15 151 9 2289 11 13 502 3'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(str(o) for o in trn_lm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH / 'tmp' / 'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH / 'tmp' / 'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH / 'tmp' / 'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH / 'tmp' / 'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH / 'tmp' / 'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH / 'tmp' / 'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 90000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = len(itos)\n",
    "vs, len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "\n",
    "### WikiText-103 conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to build an English language model (LM) for the IMDb corpus. We could start from scratch and try to learn the structure of the English language. But we use a technique called transfer learning to make this process easier. In transfer learning (a fairly recent idea for NLP) a pre-trained LM that has been trained on a large generic corpus(_like wikipedia articles_) can be used to transfer it's knowledge to a target LM and the weights can be fine-tuned.\n",
    "\n",
    "Our source LM is the WikiText-103 LM created by Stephen Merity @ Salesforce research. [Link to dataset](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n",
    "The language model for wikitext103 (AWD LSTM) has been pre-trained and the weights can be downloaded here: [Link](http://files.fast.ai/models/wt103/). Our target LM is the IMDb LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-06-28 04:49:15--  http://files.fast.ai/models/wt103/\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:49:16 (140 MB/s) - ‘data/aclImdb/models/wt103/index.html’ saved [857/857]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2018-06-28 04:49:16--  http://files.fast.ai/robots.txt\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2018-06-28 04:49:16 ERROR 404: Not Found.\n",
      "\n",
      "--2018-06-28 04:49:16--  http://files.fast.ai/models/wt103/?C=N;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=N;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:49:16 (160 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=N;O=D’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:49:16--  http://files.fast.ai/models/wt103/?C=M;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=M;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:49:16 (138 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=M;O=A’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:49:16--  http://files.fast.ai/models/wt103/?C=S;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=S;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:49:16 (134 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=S;O=A’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:49:16--  http://files.fast.ai/models/wt103/?C=D;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=D;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:49:17 (123 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=D;O=A’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:49:17--  http://files.fast.ai/models/wt103/bwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: ‘data/aclImdb/models/wt103/bwd_wt103.h5’\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  7.69MB/s    in 59s     \n",
      "\n",
      "2018-06-28 04:50:16 (7.45 MB/s) - ‘data/aclImdb/models/wt103/bwd_wt103.h5’ saved [462387687/462387687]\n",
      "\n",
      "--2018-06-28 04:50:16--  http://files.fast.ai/models/wt103/bwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: ‘data/aclImdb/models/wt103/bwd_wt103_enc.h5’\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  6.76MB/s    in 59s     \n",
      "\n",
      "2018-06-28 04:51:16 (7.43 MB/s) - ‘data/aclImdb/models/wt103/bwd_wt103_enc.h5’ saved [462387634/462387634]\n",
      "\n",
      "--2018-06-28 04:51:16--  http://files.fast.ai/models/wt103/fwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: ‘data/aclImdb/models/wt103/fwd_wt103.h5’\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  7.66MB/s    in 59s     \n",
      "\n",
      "2018-06-28 04:52:15 (7.48 MB/s) - ‘data/aclImdb/models/wt103/fwd_wt103.h5’ saved [462387687/462387687]\n",
      "\n",
      "--2018-06-28 04:52:15--  http://files.fast.ai/models/wt103/fwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: ‘data/aclImdb/models/wt103/fwd_wt103_enc.h5’\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  7.76MB/s    in 58s     \n",
      "\n",
      "2018-06-28 04:53:13 (7.61 MB/s) - ‘data/aclImdb/models/wt103/fwd_wt103_enc.h5’ saved [462387634/462387634]\n",
      "\n",
      "--2018-06-28 04:53:13--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4161252 (4.0M) [text/plain]\n",
      "Saving to: ‘data/aclImdb/models/wt103/itos_wt103.pkl’\n",
      "\n",
      "models/wt103/itos_w 100%[===================>]   3.97M  9.86MB/s    in 0.4s    \n",
      "\n",
      "2018-06-28 04:53:13 (9.86 MB/s) - ‘data/aclImdb/models/wt103/itos_wt103.pkl’ saved [4161252/4161252]\n",
      "\n",
      "--2018-06-28 04:53:13--  http://files.fast.ai/models/wt103/?C=N;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=N;O=A’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:53:14 (146 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=N;O=A’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:53:14--  http://files.fast.ai/models/wt103/?C=M;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=M;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:53:14 (186 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=M;O=D’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:53:14--  http://files.fast.ai/models/wt103/?C=S;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=S;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:53:14 (169 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=S;O=D’ saved [857/857]\n",
      "\n",
      "--2018-06-28 04:53:14--  http://files.fast.ai/models/wt103/?C=D;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: ‘data/aclImdb/models/wt103/index.html?C=D;O=D’\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-06-28 04:53:14 (153 MB/s) - ‘data/aclImdb/models/wt103/index.html?C=D;O=D’ saved [857/857]\n",
      "\n",
      "FINISHED --2018-06-28 04:53:14--\n",
      "Total wall clock time: 4m 0s\n",
      "Downloaded: 14 files, 1.7G in 3m 56s (7.50 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# wget options:\n",
    "# -nH don't create host directories\n",
    "# -r specify recursive download\n",
    "# -np don't ascend to the parent directory\n",
    "# -P get all images, etc. needed to display HTML page\n",
    "!wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.8G\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 441M Mar 29 00:31 bwd_wt103_enc.h5\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 441M Mar 29 00:34 bwd_wt103.h5\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 441M Mar 29 00:36 fwd_wt103_enc.h5\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 441M Mar 29 00:39 fwd_wt103.h5\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:49 index.html\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:49 index.html?C=D;O=A\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:53 index.html?C=D;O=D\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:49 index.html?C=M;O=A\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:53 index.html?C=M;O=D\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:53 index.html?C=N;O=A\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:49 index.html?C=N;O=D\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:49 index.html?C=S;O=A\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  857 Jun 28 04:53 index.html?C=S;O=D\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 4.0M Mar 29 00:30 itos_wt103.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {PATH}/models/wt103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained LM weights have an embedding size of 400, 1150 hidden units and just 3 layers. We need to match these values  with the target IMDB LM so that the weights can be loaded up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz, nh, nl = 400, 1150, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our pre-trained path and our pre-trained language model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = PATH / 'models' / 'wt103'\n",
    "PRE_LM_PATH = PRE_PATH / 'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map IMDb vocab to WikiText vocab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the mean of the layer0 encoder weights. This can be used to assign weights to unknown tokens when we transfer to target IMDB LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "15\n",
      "0.encoder.weight\n",
      "0.encoder_with_dropout.embed.weight\n",
      "0.rnns.0.module.weight_ih_l0\n",
      "0.rnns.0.module.bias_ih_l0\n",
      "0.rnns.0.module.bias_hh_l0\n",
      "0.rnns.0.module.weight_hh_l0_raw\n",
      "0.rnns.1.module.weight_ih_l0\n",
      "0.rnns.1.module.bias_ih_l0\n",
      "0.rnns.1.module.bias_hh_l0\n",
      "0.rnns.1.module.weight_hh_l0_raw\n",
      "0.rnns.2.module.weight_ih_l0\n",
      "0.rnns.2.module.bias_ih_l0\n",
      "0.rnns.2.module.bias_hh_l0\n",
      "0.rnns.2.module.weight_hh_l0_raw\n",
      "1.decoder.weight\n",
      "\n",
      "<class 'torch.FloatTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "(238462, 400)\n",
      "<class 'numpy.ndarray'>\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "# ==================================== START DEBUG ====================================\n",
    "\n",
    "# specialized container datatypes providing alternatives to Python’s general purpose built-in containers, dict.\n",
    "# dict subclass that remembers the order entries were added.\n",
    "\n",
    "# In other words,\n",
    "# a regular dict does not track the insertion order, and iterating over it produces the values in an arbitrary order.\n",
    "# In an OrderedDict, by contrast, the order the items are inserted is remembered and used when creating an iterator.\n",
    "print(type(wgts))\n",
    "\n",
    "print(len(wgts))\n",
    "for k in wgts.keys():\n",
    "    print(k)\n",
    "\n",
    "tmp_wgts = wgts['0.encoder.weight']\n",
    "print(f'\\n{type(tmp_wgts)}' )\n",
    "\n",
    "tmp_enc_wgts = to_np(tmp_wgts)\n",
    "print( type(to_np(tmp_wgts)) )\n",
    "\n",
    "# pre-trained LM weights have an embedding size of 400\n",
    "print( tmp_enc_wgts.shape )\n",
    "\n",
    "tmp_row_m = tmp_enc_wgts.mean(0) # returns the average of the array elements along axis 0\n",
    "print( type(tmp_row_m) ) # numpy.ndarray\n",
    "print( tmp_row_m.shape ) # shape: (400,)\n",
    "\n",
    "# ==================================== END DEBUG ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight']) # converts np.ndarray from torch.FloatTensor.output shape: (238462, 400)\n",
    "row_m = enc_wgts.mean(0) # returns the average of the array elements along axis 0. output shape: (400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2 = pickle.load( (PRE_PATH / 'itos_wt103.pkl').open('rb') )\n",
    "stoi2 = collections.defaultdict(lambda: -1, { v: k for k, v in enumerate(itos2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "238462\n",
      "0: _unk_\n",
      "1: _pad_\n",
      "2: the\n",
      "3: ,\n",
      "4: .\n",
      "5: of\n",
      "6: and\n",
      "7: in\n",
      "8: to\n",
      "9: a\n",
      "<class 'collections.defaultdict'>\n",
      "238462\n",
      "-1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# ==================================== START DEBUG ====================================\n",
    "\n",
    "print( type(itos2) )\n",
    "print(len(itos2))\n",
    "\n",
    "i = 0\n",
    "for k, v in enumerate(itos2):\n",
    "    i = i + 1\n",
    "    if i <= 10:\n",
    "        print(f'{k}: {v}')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print( type(stoi2) )\n",
    "print( len(stoi2) )\n",
    "print( stoi2['On'] ) # returns -1 (because the vocab is not found)\n",
    "print( stoi2['the'] ) # returns 2\n",
    "\n",
    "# ==================================== END DEBUG ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to transfer the knowledge from WikiText to the IMDb LM, we match up the vocab words and their indexes. We use the `defaultdict` container once again, to assign mean weights to unknown IMDb tokens that do not exist in WikiText-103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w = np.zeros((vs, em_sz), dtype=np.float32) # shape: (60002, 400)\n",
    "\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r >= 0 else row_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now overwrite the weights into the `wgts` OrderedDict.\n",
    "The decoder module, which we will explore in detail is also loaded with the same weights due to an idea called weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) # weird thing with how we do embedding dropout\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weights prepared, we are ready to create and start training our new IMDb language PyTorch model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fairly straightforward to create a new language model using the fastai library. Like every other lesson, our model will have a backbone and a custom head. The backbone in our case is the IMDb LM pre-trained with WikiText and the custom head is a linear classifier. In this section we will focus on the backbone LM and the next section will talk about the classifier custom head.\n",
    "\n",
    "bptt (*also known traditionally in NLP LM as ngrams*) in fastai LMs is approximated to a std. deviation around 70, by perturbing the sequence length on a per-batch basis. This is akin to shuffling our data in computer vision, only that in NLP we cannot shuffle inputs and we have to maintain statefulness. \n",
    "\n",
    "Since we are predicting words using ngrams, we want our next batch to line up with the end-points of the previous mini-batch's items. batch-size is constant and but the fastai library expands and contracts bptt each mini-batch using a clever stochastic implementation of a batch. (original credits attributed to [Smerity](https://twitter.com/jeremyphoward/status/980227258395770882))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "bptt = 70\n",
    "bs = 52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the LM is to learn to predict a word/token given a preceeding set of words(tokens). We take all the movie reviews in both the 90k training set and 10k validation set and concatenate them to form long strings of tokens. In fastai, we use the `LanguageModelLoader` to create a data loader which makes it easy to create and use bptt sized mini batches. The  `LanguageModelLoader` takes a concatenated string of tokens and returns a loader.\n",
    "\n",
    "We have a special modeldata object class for LMs called `LanguageModelData` to which we can pass the training and validation loaders and get in return the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
