{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Fast.ai we have introduced a new module called fastai.text which replaces the torchtext library that was used in our 2018 dl1 course. The fastai.text module also supersedes the fastai.nlp library but retains many of the key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fastai.text module introduces several custom tokens.\n",
    "\n",
    "We need to download the IMDB large movie reviews from this site: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "Direct link : [Link](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and untar it into the PATH location. We use pathlib which makes directory traveral a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===================================== (START) Download IMDb data =====================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/aclImdb\n"
     ]
    }
   ],
   "source": [
    "%mkdir data/aclImdb\n",
    "%cd data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#6acf06 79MiB/80MiB(99%) CN:1 DL:\u001b[32m14MiB\u001b[0m]\u001b[0m                          \n",
      "06/26 15:59:49 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /home/ubuntu/data/aclImdb/aclImdb_v1.tar.gz\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "6acf06|\u001b[1;32mOK\u001b[0m  |    14MiB/s|/home/ubuntu/data/aclImdb/aclImdb_v1.tar.gz\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    }
   ],
   "source": [
    "!aria2c --file-allocation=none -c -x 5 -s 5 http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxf aclImdb_v1.tar.gz -C ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "%rm data/aclImdb/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv data/aclImdb/ data/aclImdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv data/aclImdb2/aclImdb data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf data/aclImdb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\r\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4.0K Jun 26  2011 .\r\n",
      "drwxrwxr-x 8 ubuntu ubuntu 4.0K Jun 26 16:17 ..\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 882K Jun 11  2011 imdbEr.txt\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 827K Apr 12  2011 imdb.vocab\r\n",
      "-rw-r--r-- 1 ubuntu ubuntu 4.0K Jun 26  2011 README\r\n",
      "drwxr-xr-x 4 ubuntu ubuntu 4.0K Jun 26 16:02 test\r\n",
      "drwxr-xr-x 5 ubuntu ubuntu 4.0K Jun 26 16:02 train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**===================================== (END) Download IMDb data =====================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH = Path('data/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb  dogscats  dogscats.zip  imdb_clas  pascal  spellbee\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_PATH = Path('data/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb  dogscats  dogscats.zip  imdb_clas  imdb_lm  pascal  spellbee\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDb dataset has 3 classes; positive, negative and unsupervised(sentiment is unknown).\n",
    "There are 75k training reviews(12.5k pos, 12.5k neg, 50k unsup)\n",
    "There are 25k validation reviews(12.5k pos, 12.5k neg & no unsup)\n",
    "\n",
    "Refer to the README file in the IMDb corpus for further information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\r\n",
      "\r\n",
      "Overview\r\n",
      "\r\n",
      "This dataset contains movie reviews along with their associated binary\r\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\r\n",
      "sentiment classification. This document outlines how the dataset was\r\n",
      "gathered, and how to use the files provided. \r\n",
      "\r\n",
      "Dataset \r\n",
      "\r\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\r\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\r\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\r\n",
      "documents for unsupervised learning. \r\n",
      "\r\n",
      "In the entire collection, no more than 30 reviews are allowed for any\r\n",
      "given movie because reviews for the same movie tend to have correlated\r\n",
      "ratings. Further, the train and test sets contain a disjoint set of\r\n",
      "movies, so no significant performance is obtained by memorizing\r\n",
      "movie-unique terms and their associated with observed labels.  In the\r\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\r\n",
      "more neutral ratings are not included in the train/test sets. In the\r\n",
      "unsupervised set, reviews of any rating are included and there are an\r\n",
      "even number of reviews > 5 and <= 5.\r\n",
      "\r\n",
      "Files\r\n",
      "\r\n",
      "There are two top-level directories [train/, test/] corresponding to\r\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\r\n",
      "the reviews with binary labels positive and negative. Within these\r\n",
      "directories, reviews are stored in text files named following the\r\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n",
      "the star rating for that review on a 1-10 scale. For example, the file\r\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\r\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\r\n",
      "omitted for this portion of the dataset.\r\n",
      "\r\n",
      "We also include the IMDb URLs for each review in a separate\r\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n",
      "are unable to link directly to the review, but only to the movie's\r\n",
      "review page.\r\n",
      "\r\n",
      "In addition to the review text files, we include already-tokenized bag\r\n",
      "of words (BoW) features that were used in our experiments. These \r\n",
      "are stored in .feat files in the train/test directories. Each .feat\r\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n",
      "data.  The feature indices in these files start from 0, and the text\r\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n",
      "(the) appears 7 times in that review.\r\n",
      "\r\n",
      "LIBSVM page for details on .feat file format:\r\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n",
      "\r\n",
      "We also include [imdbEr.txt] which contains the expected rating for\r\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n",
      "rating is a good way to get a sense for the average polarity of a word\r\n",
      "in the dataset.\r\n",
      "\r\n",
      "Citing the dataset\r\n",
      "\r\n",
      "When using this dataset please cite our ACL 2011 paper which\r\n",
      "introduces it. This paper also contains classification results which\r\n",
      "you may want to compare against.\r\n",
      "\r\n",
      "\r\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\r\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n",
      "  month     = {June},\r\n",
      "  year      = {2011},\r\n",
      "  address   = {Portland, Oregon, USA},\r\n",
      "  publisher = {Association for Computational Linguistics},\r\n",
      "  pages     = {142--150},\r\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n",
      "}\r\n",
      "\r\n",
      "References\r\n",
      "\r\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n",
      "636-659.\r\n",
      "\r\n",
      "Contact\r\n",
      "\r\n",
      "For questions/comments/corrections please contact Andrew Maas\r\n",
      "amaas@cs.stanford.edu\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos', 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path):\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts, trn_labels = get_texts(PATH / 'train')\n",
    "val_texts, val_labels = get_texts(PATH / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 25000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random permutation numpy array to shuffle the text reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({ 'text': trn_texts, 'labels': trn_labels }, columns=col_names)\n",
    "df_val = pd.DataFrame({ 'text': val_texts, 'labels': val_labels }, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of filmmakers (College Students?) deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sequels have a nasty habit of being disappoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a future society, the military component do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Imagine Albert Finney, one of the great ham bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I bought this DVD for $2.00 at the local varie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       2  A group of filmmakers (College Students?) deci...\n",
       "1       0  Sequels have a nasty habit of being disappoint...\n",
       "2       1  In a future society, the military component do...\n",
       "3       2  Imagine Albert Finney, one of the great ham bo...\n",
       "4       2  I bought this DVD for $2.00 at the local varie..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# View train df\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Every year there's one can't-miss much-anticip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't usually like this sort of movie but wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great movie in a Trainspotting style... Being ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>New rule. Nobody is allowed to make any more Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I saw this movie (unfortunately) because it wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       1  Every year there's one can't-miss much-anticip...\n",
       "1       1  I don't usually like this sort of movie but wa...\n",
       "2       1  Great movie in a Trainspotting style... Being ...\n",
       "3       0  New rule. Nobody is allowed to make any more Z...\n",
       "4       0  I saw this movie (unfortunately) because it wa..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# View validation df\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas dataframe is used to store text data in a newly evolving standard format of label followed by text columns. This was influenced by a paper by Yann LeCun (LINK REQUIRED). Fastai adopts this new format for NLP datasets. In the case of IMDB, there is only one text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove everything that has a label of 2 `df_trn['labels'] != 2` because label of 2 is \"unsupervised\" and we can’t use it.\n",
    "df_trn[df_trn['labels'] != 2].to_csv(CLAS_PATH / 'train.csv', header=False, index=False)\n",
    "\n",
    "df_val.to_csv(CLAS_PATH / 'test.csv', header=False, index=False)\n",
    "\n",
    "(CLAS_PATH / 'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the data for the Language Model(LM). The LM's goal is to learn the structure of the English language. It learns language by trying to predict the next word given a set of previous words(ngrams). Since the LM does not classify reviews, the labels can be ignored.\n",
    "\n",
    "The LM can benefit from all the textual data and there is no need to exclude the unsup/unclassified movie reviews.\n",
    "\n",
    "We first concat all the train(pos/neg/unsup = **75k**) and test(pos/neg=**25k**) reviews into a big chunk of **100k** reviews. And then we use sklearn splitter to divide up the 100k texts into 90% training and 10% validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts, val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_texts, val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({ 'text': trn_texts, 'labels': [0] * len(trn_texts) }, columns=col_names)\n",
    "df_val = pd.DataFrame({ 'text': val_texts, 'labels': [0] * len(val_texts) }, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH / 'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH / 'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we start cleaning up the messy text. There are 2 main activities we need to perform:\n",
    "\n",
    "1. Clean up extra spaces, tab chars, new line chars and other characters and replace them with standard ones.\n",
    "2. Use the awesome [spaCy](http://spacy.io) library to tokenize the data. Since spaCy does not provide a parallel/multicore version of the tokenizer, the fastai library adds this functionality. This parallel version uses all the cores of your CPUs and runs much faster than the serial version of the spacy tokenizer.\n",
    "\n",
    "Tokenization is the process of splitting the text into separate tokens so that each token can be assigned a unique index. This means we can convert the text into integer indexes our models can use.\n",
    "\n",
    "We use an appropriate `chunksize` as the tokenization process is memory intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 24000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we pass it to spaCy, we will write a simple fixup function which is each time we have looked at different datasets (about a dozen in building this), every one had different weird things that needed to be replaced. So here are all the ones we have come up with so far, and hopefully this will help you out as well. All the entities are HTML unescaped and there are bunch more things we replace. Have a look at the result of running this on text that you put in and make sure there's no more weird tokens in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:, range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls + 1, len(df.columns)):\n",
    "        texts += f' {FLD} {i - n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    \n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    \n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH / 'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH / 'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-51e26f4b98a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtok_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-bfe25ce1655c>\u001b[0m in \u001b[0;36mget_all\u001b[0;34m(df, n_lbls)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtok_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lbls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtok\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtok_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-d4a58d702615>\u001b[0m in \u001b[0;36mget_texts\u001b[0;34m(df, n_lbls)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_all_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_by_cores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/courses/dl2/fastai/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_br\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<\\s*br\\s*/?>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "# tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix spaCy issue above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 81.7MB/s ta 0:00:01  7% |██▎                             | 2.6MB 611kB/s eta 0:00:57\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Re-run these 2 lines of codes.\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH / 'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 66000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tok_trn), len(tok_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'xbos',\n",
       " 'xfld',\n",
       " '1',\n",
       " '\"',\n",
       " 'challenge',\n",
       " 'to',\n",
       " 'be',\n",
       " 'free',\n",
       " '\"',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'films',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 't_up',\n",
       " 'vhs',\n",
       " 'tapes',\n",
       " 'that',\n",
       " 'i',\n",
       " 'owned',\n",
       " '.',\n",
       " 'i',\n",
       " 'had',\n",
       " \"n't\",\n",
       " 'seen',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'years',\n",
       " ',',\n",
       " 'so',\n",
       " 'yesterday',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'stick',\n",
       " 'the',\n",
       " 'tape',\n",
       " 'in',\n",
       " 'and',\n",
       " 'watch',\n",
       " 'it',\n",
       " '.',\n",
       " 'wow',\n",
       " '.',\n",
       " 'the',\n",
       " 'story',\n",
       " 'is',\n",
       " 'as',\n",
       " 'powerful',\n",
       " 'now',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'think',\n",
       " 'now',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'older',\n",
       " 'i',\n",
       " 'can',\n",
       " 'better',\n",
       " 'apreciate',\n",
       " 'the',\n",
       " 'values',\n",
       " 'that',\n",
       " 'are',\n",
       " 'implanted',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '.',\n",
       " '(',\n",
       " 'self',\n",
       " '-',\n",
       " 'reliance',\n",
       " ',',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'love',\n",
       " 'of',\n",
       " 'nature',\n",
       " ')',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " '\"',\n",
       " 'b',\n",
       " '\"',\n",
       " 'movie',\n",
       " ',',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sure',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'one',\n",
       " 'that',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'remember',\n",
       " 'for',\n",
       " 'years',\n",
       " ',',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'you',\n",
       " 'see',\n",
       " 'it',\n",
       " 'as',\n",
       " 'a',\n",
       " 'child',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 66000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_labels), len(trn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Result\n",
    "\n",
    "Beginning of the stream token (`xbos`), beginning of field number 1 token (`xfld 1`), and tokenized text. You'll see that the punctuation is on whole now a separate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n xbos xfld 1 \" challenge to be free \" was one of the first films i saw as a child . it was also one of the first t_up vhs tapes that i owned . i had n\\'t seen the movie in years , so yesterday i decided to stick the tape in and watch it . wow . the story is as powerful now as it was the first time i saw it . i think now that i am older i can better apreciate the values that are implanted in the movie . ( self - reliance , the value of freedom , and the love of nature ) it is a \" b \" movie , to be sure , but it \\'s one that you \\'ll remember for years , especially if you see it as a child .'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n xbos xfld 1 i remember early in the year 2000 listening to a local am talk radio station in los angeles ( i believe it was phil hendrie ) . the topic of the hour was \" what \\'s your favorite tv show currently on the air ? \" for the next hour i heard a lot of references to two shows : \" malcolm in the middle \" and this other show , \" popular . \" i could n\\'t believe what i was hearing . i had already seen \" malcolm \" and knew it was hilarious . but what about \" popular ? \" \\n\\n the commercials made it look like a typical weekly teeny - bop high school show , a la \" saved by the bell , \" but in prime - time ( lest we forget zach did go to college in the evenings though ) . the amount of praise i heard for \" popular \" made me check it out . \\n\\n the advertisements were wrong . \\n\\n what i found in this show was a sleek , hilarious , dramatic , clever , filled to the brim with 80 \\'s references show not about kids in high school today ( circa 1999 - 2000 ) , but if kids from my generation were to go to high school today ( yes , i went to high school in the 80 \\'s ) . \\n\\n i think the failure of \" popular \" was the advertising i referred to earlier for the show ... it was going after kids in high school for ratings . but the show was made for ( i feel ) an entirely different generation . if only the ones it was made for had watched tk_rep 4 . it would probably still be on . \\n\\n this is one of the few programs i have ever seen that managed to balance outright slapstick comedy with serious drama ... and it did it so well in my opinion that i seriously mourned the loss of the show when it was canceled . \\n\\n my favorite season two moments - \\n\\n 1 ) mary cherry \\'s ( leslie grossman ) audition for judges , including jim j. bullock ( way before \" american idol \" became a household name ) . \" rock me amadeus , by t_up falco ! \" \\n\\n 2 ) nicole julian \\'s ( tammy lynn michaels ) \" rise \" to class president ( one of the most clever and complex episodes ever written for television -- flat - out incredible comedy ) . \\n\\n 3 ) every scene with michelle phillips ( \" abra - abra - cadabra tk_rep 4 ! \") . \\n\\n 4 ) the \" gay \" episode about bobbi glass ( diane delano ) - it gave me chills and again proved how the writers were almost magically able to juggle comedy and drama every week . \\n\\n the casting of the show was simply brilliant . everyone did an incredible job and i am pleased to see several of the cast riding other roads to fame as of 2008 ( christopher gorham on \" jake 2.0 \" and then the successful \" ugly betty \" ) . \\n\\n the only thing i can pray for at this point are either reruns ( lifetime or oxygen i \\'m guessing ... but maybe comedy central ? ) or , * gasp * -- a t_up dvd release . then a lot more people can have a chance to see a show that i firmly believe should have been \" popular . \" \\n\\n * t_up edit * both seasons are on t_up dvd now . there really is a god ! \\n\\n my personal thanks to the callers of that los angeles am talk radio program who touted the praises of this show . you got it right , and i \\'m glad i saw of it what i did .'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH / 'tmp' / 'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH / 'tmp' / 'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load it back up later\n",
    "tok_trn = np.load(LM_PATH / 'tmp' / 'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH / 'tmp' / 'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 252M\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 219M Jun 27 10:49 tok_trn.npy\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  34M Jun 27 10:49 tok_val.npy\r\n"
     ]
    }
   ],
   "source": [
    "# DEBUG - check serialized numpy files are created\n",
    "!ls -lh {LM_PATH}/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeracalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
